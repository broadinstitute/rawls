package org.broadinstitute.dsde.rawls

import java.io.StringReader
import java.net.InetAddress
import java.util.concurrent.TimeUnit

import akka.actor.ActorSystem
import akka.http.scaladsl.Http
import akka.stream.ActorMaterializer
import cats.effect._
import cats.implicits._
import com.codahale.metrics.SharedMetricRegistries
import com.google.api.client.googleapis.auth.oauth2.GoogleClientSecrets
import com.google.api.client.json.jackson2.JacksonFactory
import com.google.pubsub.v1.ProjectTopicName
import com.readytalk.metrics.{StatsDReporter, WorkbenchStatsD}
import com.typesafe.config.{Config, ConfigFactory, ConfigObject}
import com.typesafe.scalalogging.LazyLogging
import io.chrisdavenport.log4cats.Logger
import io.chrisdavenport.log4cats.slf4j.Slf4jLogger
import net.ceedubs.ficus.Ficus._
import org.broadinstitute.dsde.rawls.config._
import slick.basic.DatabaseConfig
import slick.jdbc.JdbcProfile
import org.broadinstitute.dsde.rawls.dataaccess._
import org.broadinstitute.dsde.rawls.genomics.GenomicsService
import org.broadinstitute.dsde.rawls.google.HttpGooglePubSubDAO
import org.broadinstitute.dsde.rawls.model._
import org.broadinstitute.dsde.rawls.monitor._
import org.broadinstitute.dsde.rawls.statistics.StatisticsService
import org.broadinstitute.dsde.rawls.status.StatusService
import org.broadinstitute.dsde.rawls.user.UserService
import org.broadinstitute.dsde.rawls.util.ScalaConfig._
import org.broadinstitute.dsde.rawls.util._
import org.broadinstitute.dsde.rawls.webservice._
import org.broadinstitute.dsde.rawls.workspace.{WorkspaceService, WorkspaceServiceConfig}
import org.broadinstitute.dsde.workbench.google.GoogleCredentialModes.Json
import org.broadinstitute.dsde.workbench.google.HttpGoogleBigQueryDAO
import org.broadinstitute.dsde.workbench.google2._
import org.broadinstitute.dsde.workbench.model.google.{GcsBucketName, GoogleProject}
import org.broadinstitute.dsde.workbench.util.ExecutionContexts
import org.http4s.Uri
import org.http4s.client.blaze.BlazeClientBuilder

import scala.collection.JavaConverters._
import scala.concurrent.ExecutionContext
import scala.concurrent.ExecutionContext.Implicits.global
import scala.concurrent.duration._
import scala.util.{Failure, Success}
import net.ceedubs.ficus.Ficus._
import org.apache.commons.io.FileUtils
import org.broadinstitute.dsde.workbench.model.google.GcsBucketName

object Boot extends IOApp with LazyLogging {
  override def run(
                    args: List[String]
                  ): IO[ExitCode] =
    startup() *> ExitCode.Success.pure[IO]

  private def startup(): IO[Unit] = {
    implicit val log4CatsLogger: _root_.io.chrisdavenport.log4cats.Logger[IO] = Slf4jLogger.getLogger[IO]

    // version.conf is generated by sbt
    val conf = ConfigFactory.parseResources("version.conf").withFallback(ConfigFactory.load())
    val gcsConfig = conf.getConfig("gcs")

    // we need an ActorSystem to host our application in
    implicit val system = ActorSystem("rawls")
    implicit val materializer = ActorMaterializer()

    val slickDataSource = DataSource(DatabaseConfig.forConfig[JdbcProfile]("slick", conf))

    val liquibaseConf = conf.getConfig("liquibase")
    val liquibaseChangeLog = liquibaseConf.getString("changelog")
    val initWithLiquibase = liquibaseConf.getBoolean("initWithLiquibase")

    val changelogParams = Map("gcs:appsDomain" -> gcsConfig.getString("appsDomain"))

    if(initWithLiquibase) {
      slickDataSource.initWithLiquibase(liquibaseChangeLog, changelogParams)
    }

    val metricsConf = conf.getConfig("metrics")
    val metricsPrefix = {
      val basePrefix = metricsConf.getString("prefix")
      metricsConf.getBooleanOption("includeHostname") match {
        case Some(true) =>
          val hostname = InetAddress.getLocalHost().getHostName()
          basePrefix + "." + hostname
        case _ => basePrefix
      }
    }

    if (metricsConf.getBooleanOption("enabled").getOrElse(false)) {
      metricsConf.getObjectOption("reporters") match {
        case Some(configObject) =>
          configObject.entrySet.asScala.map(_.toTuple).foreach {
            case ("statsd", conf: ConfigObject) =>
              val statsDConf = conf.toConfig
              startStatsDReporter(
                statsDConf.getString("host"),
                statsDConf.getInt("port"),
                statsDConf.getDuration("period"),
                apiKey = statsDConf.getStringOption("apiKey"))
            case (other, _) =>
              logger.warn(s"Unknown metrics backend: $other")
          }
        case None => logger.info("No metrics reporters defined")
      }
    } else {
      logger.info("Metrics reporting is disabled.")
    }

    val jsonFactory = JacksonFactory.getDefaultInstance
    val clientSecrets = GoogleClientSecrets.load(jsonFactory, new StringReader(gcsConfig.getString("secrets")))
    val clientEmail = gcsConfig.getString("serviceClientEmail")
    val hammCromwellMetadataConfig = gcsConfig.getConfig("hamm-cromwell-metadata")
    val serviceProject = gcsConfig.getString("serviceProject")
    val hammCromwellMetadata = HammCromwellMetadata(
      GcsBucketName(hammCromwellMetadataConfig.getString("bucket-name")),
      ProjectTopicName.of(serviceProject, hammCromwellMetadataConfig.getString("topic-name "))
    )

    initAppDependencies[IO](conf).use { appDependencies =>
      val gcsDAO = new HttpGoogleServicesDAO(
        false,
        clientSecrets,
        clientEmail,
        gcsConfig.getString("subEmail"),
        gcsConfig.getString("pathToPem"),
        gcsConfig.getString("appsDomain"),
        gcsConfig.getString("groupsPrefix"),
        gcsConfig.getString("appName"),
        gcsConfig.getInt("deletedBucketCheckSeconds"),
        serviceProject,
        gcsConfig.getString("tokenEncryptionKey"),
        gcsConfig.getString("tokenSecretsJson"),
        gcsConfig.getString("billingPemEmail"),
        gcsConfig.getString("pathToBillingPem"),
        gcsConfig.getString("billingEmail"),
        gcsConfig.getInt("bucketLogsMaxAge"),
        hammCromwellMetadata = hammCromwellMetadata,
        googleStorageService = appDependencies.googleStorageService,
        googleServiceHttp = appDependencies.googleServiceHttp,
        topicAdmin = appDependencies.topicAdmin,
        workbenchMetricBaseName = metricsPrefix,
        proxyNamePrefix = gcsConfig.getStringOr("proxyNamePrefix", "")
      )

      val pubSubDAO = new HttpGooglePubSubDAO(
        clientEmail,
        gcsConfig.getString("pathToPem"),
        gcsConfig.getString("appName"),
        gcsConfig.getString("serviceProject"),
        workbenchMetricBaseName = metricsPrefix
      )

      val bigQueryDAO = new HttpGoogleBigQueryDAO(
        gcsConfig.getString("appName"),
        Json(gcsConfig.getString("bigQueryJson")),
        metricsPrefix
      )

      val samConfig = conf.getConfig("sam")
      val samDAO = new HttpSamDAO(
        samConfig.getString("server"),
        gcsDAO.getBucketServiceAccountCredential
      )

      enableServiceAccount(gcsDAO, samDAO)

      system.registerOnTermination {
        slickDataSource.databaseConfig.db.shutdown
      }

      val executionServiceConfig = conf.getConfig("executionservice")
      val submissionTimeout = util.toScalaDuration(
        executionServiceConfig.getDuration("workflowSubmissionTimeout")
      )

      val executionServiceAbortServers: Map[String, ExecutionServiceDAO] =
        executionServiceConfig
          .getObjectOption("abortServers")
          .map(_.entrySet().asScala.map { entry =>
            val (strName, strHostname) = entry.toTuple
            strName -> new HttpExecutionServiceDAO(
              strHostname.unwrapped.toString,
              metricsPrefix
            )
          }.toMap)
          .getOrElse(Map.empty)

      val executionServiceServers: Set[ClusterMember] = executionServiceConfig
        .getObject("readServers")
        .entrySet()
        .asScala
        .map { entry =>
          val (strName, strHostname) = entry.toTuple
          ClusterMember(
            ExecutionServiceId(strName),
            new HttpExecutionServiceDAO(
              strHostname.unwrapped.toString,
              metricsPrefix
            ),
            executionServiceAbortServers.get(strName)
          )
        }
        .toSet

      val executionServiceSubmitServers: Set[ClusterMember] =
        executionServiceConfig
          .getObject("submitServers")
          .entrySet()
          .asScala
          .map { entry =>
            val (strName, strHostname) = entry.toTuple
            ClusterMember(
              ExecutionServiceId(strName),
              new HttpExecutionServiceDAO(
                strHostname.unwrapped.toString,
                metricsPrefix
              ),
              executionServiceAbortServers.get(strName)
            )
          }
          .toSet

      val shardedExecutionServiceCluster: ExecutionServiceCluster =
        new ShardedHttpExecutionServiceCluster(
          executionServiceServers,
          executionServiceSubmitServers,
          slickDataSource
        )
      val projectOwners =
        gcsConfig.getStringList("projectTemplate.owners").asScala
      val projectEditors =
        gcsConfig.getStringList("projectTemplate.editors").asScala
      val projectServices =
        gcsConfig.getStringList("projectTemplate.services").asScala
      val projectOwnerGrantableRoles =
        gcsConfig.getStringList("projectTemplate.ownerGrantableRoles")
      val requesterPaysRole = gcsConfig.getString("requesterPaysRole")
      val projectTemplate = ProjectTemplate(
        Map("roles/owner" -> projectOwners, "roles/editor" -> projectEditors),
        projectServices
      )

      val notificationDAO = new PubSubNotificationDAO(
        pubSubDAO,
        gcsConfig.getString("notifications.topicName")
      )
      val marthaConfig = conf.getConfig("martha")
      val dosResolver = new MarthaDosResolver(marthaConfig.getString("baseUrl"))
      val userServiceConstructor: (UserInfo) => UserService =
        UserService.constructor(
          slickDataSource,
          gcsDAO,
          notificationDAO,
          samDAO,
          projectOwnerGrantableRoles.asScala,
          requesterPaysRole
        )
      val genomicsServiceConstructor: (UserInfo) => GenomicsService =
        GenomicsService.constructor(slickDataSource, gcsDAO)
      val statisticsServiceConstructor: (UserInfo) => StatisticsService =
        StatisticsService.constructor(slickDataSource, gcsDAO)
      val submissionCostService: SubmissionCostService =
        SubmissionCostService.constructor(
          gcsConfig.getString("billingExportTableName"),
          gcsConfig.getString("serviceProject"),
          bigQueryDAO
        )

      val methodRepoDAO = new HttpMethodRepoDAO(
        MethodRepoConfig.apply[Agora.type](conf.getConfig("agora")),
        MethodRepoConfig.apply[Dockstore.type](conf.getConfig("dockstore")),
        metricsPrefix
      )

      val maxActiveWorkflowsTotal = conf.getInt(
        "executionservice.maxActiveWorkflowsPerServer"
      ) * executionServiceServers.size
      val maxActiveWorkflowsPerUser = maxActiveWorkflowsTotal / conf.getInt(
        "executionservice.activeWorkflowHogFactor"
      )
      val useWorkflowCollectionField =
        conf.getBoolean("executionservice.useWorkflowCollectionField")
      val useWorkflowCollectionLabel =
        conf.getBoolean("executionservice.useWorkflowCollectionLabel")
      val defaultBackend: CromwellBackend = CromwellBackend(conf.getString("executionservice.defaultBackend"))

      if (conf.getBooleanOption("backRawls").getOrElse(false)) {
        logger.info("This instance has been marked as BACK. Booting monitors...")
        BootMonitors.bootMonitors(
          system,
          conf,
          slickDataSource,
          gcsDAO,
          samDAO,
          pubSubDAO,
          methodRepoDAO,
          dosResolver,
          shardedExecutionServiceCluster,
          maxActiveWorkflowsTotal,
          maxActiveWorkflowsPerUser,
          userServiceConstructor,
          projectTemplate,
          metricsPrefix,
          requesterPaysRole,
          useWorkflowCollectionField,
          useWorkflowCollectionLabel,
          defaultBackend
        )
      } else
        logger.info(
          "This instance has been marked as FRONT. Monitors will not be booted..."
        )

      val healthMonitor = system.actorOf(
        HealthMonitor
          .props(
            slickDataSource,
            gcsDAO,
            pubSubDAO,
            methodRepoDAO,
            samDAO,
            executionServiceServers.map(c => c.key -> c.dao).toMap,
            groupsToCheck = Seq(gcsDAO.adminGroupName, gcsDAO.curatorGroupName),
            topicsToCheck = Seq(gcsConfig.getString("notifications.topicName")),
            bucketsToCheck = Seq(gcsDAO.tokenBucketName)
          )
          .withDispatcher("health-monitor-dispatcher"),
        "health-monitor"
      )
      logger.info("Starting health monitor...")
      system.scheduler.schedule(
        10 seconds,
        1 minute,
        healthMonitor,
        HealthMonitor.CheckAll
      )

      val statusServiceConstructor: () => StatusService = () =>
        StatusService.constructor(healthMonitor)

      val workspaceServiceConfig = WorkspaceServiceConfig(
        conf.getBoolean("submissionmonitor.trackDetailedSubmissionMetrics"),
        gcsConfig.getString("groupsPrefix")
      )

      val service = new RawlsApiServiceImpl(
        WorkspaceService.constructor(
          slickDataSource,
          methodRepoDAO,
          shardedExecutionServiceCluster,
          conf.getInt("executionservice.batchSize"),
          gcsDAO,
          samDAO,
          notificationDAO,
          userServiceConstructor,
          genomicsServiceConstructor,
          maxActiveWorkflowsTotal,
          maxActiveWorkflowsPerUser,
          workbenchMetricBaseName = metricsPrefix,
          submissionCostService,
          workspaceServiceConfig
        ),
        userServiceConstructor,
        genomicsServiceConstructor,
        statisticsServiceConstructor,
        statusServiceConstructor,
        shardedExecutionServiceCluster,
        ApplicationVersion(
          conf.getString("version.git.hash"),
          conf.getString("version.build.number"),
          conf.getString("version.version")
        ),
        clientSecrets.getDetails.getClientId,
        submissionTimeout,
        metricsPrefix,
        samDAO,
        conf.as[SwaggerConfig]("swagger")
      )

      for {
        _ <- IO.fromFuture(IO(Http().bindAndHandle(service.route, "0.0.0.0", 8080))).recover {
          case t: Throwable =>
            logger.error("FATAL - failure starting http server", t)
            throw t
        }
      } yield ()
    }
  }

  /**
   * Enables the rawls service account in ldap. Allows service to service auth through the proxy.
   * @param gcsDAO
   */
  def enableServiceAccount(gcsDAO: HttpGoogleServicesDAO, samDAO: HttpSamDAO): Unit = {
    val credential = gcsDAO.getBucketServiceAccountCredential
    val serviceAccountUserInfo = UserInfo.buildFromTokens(credential)

    val registerServiceAccountFuture = samDAO.registerUser(serviceAccountUserInfo)

    registerServiceAccountFuture.onFailure {
      // this is logged as a warning because almost always the service account is already enabled
      // so this is a problem only the first time rawls is started with a new service account
      case t: Throwable => logger.warn("error enabling service account", t)
    }
  }

  def startStatsDReporter(host: String, port: Int, period: java.time.Duration, registryName: String = "default", apiKey: Option[String] = None): Unit = {
    logger.info(s"Starting statsd reporter writing to [$host:$port] with period [${period.toMillis} ms]")
    val reporter = StatsDReporter.forRegistry(SharedMetricRegistries.getOrCreate(registryName))
      .prefixedWith(apiKey.orNull)
      .convertRatesTo(TimeUnit.SECONDS)
      .convertDurationsTo(TimeUnit.MILLISECONDS)
      .build(WorkbenchStatsD(host, port))
    reporter.start(period.toMillis, period.toMillis, TimeUnit.MILLISECONDS)
  }

  def initAppDependencies[F[_]: ConcurrentEffect: Timer: Logger: ContextShift](config: Config)(implicit executionContext: ExecutionContext): cats.effect.Resource[F, AppDependencies[F]] = {
    val gcsConfig = config.getConfig("gcs")
    val serviceProject = GoogleProject(gcsConfig.getString("serviceProject"))
    val pathToCredentialJson = gcsConfig.getString("pathToCredentialJson")
    val googleApiUri = Uri.unsafeFromString(gcsConfig.getString("google-api-uri"))
    val metadataNotificationConfig = NotificationCreaterConfig(pathToCredentialJson, googleApiUri)

    for {
      blockingEc <- ExecutionContexts.fixedThreadPool[F](256) //scala.concurrent.blocking has default max extra thread number 256, so use this number to start with
      googleStorage <- GoogleStorageService.resource[F](pathToCredentialJson, blockingEc, Some(serviceProject))
      httpClient <- BlazeClientBuilder(executionContext).resource
      googleServiceHttp <- GoogleServiceHttp.withRetryAndLogging(httpClient, metadataNotificationConfig)
      topicAdmin <- GoogleTopicAdmin.fromCredentialPath(pathToCredentialJson)
    } yield AppDependencies[F](googleStorage, googleServiceHttp, topicAdmin)
  }
}

// Any resources need clean up should be put in AppDependencies
final case class AppDependencies[F[_]](
  googleStorageService: GoogleStorageService[F],
  googleServiceHttp: GoogleServiceHttp[F],
  topicAdmin: GoogleTopicAdmin[F])