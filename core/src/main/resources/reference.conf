clone-workspace-file-transfer-monitor-dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    fixed-pool-size = 32
  }
  throughput = 1
}

submission-monitor-dispatcher {
  # Dispatcher is the name of the event-based dispatcher
  type = Dispatcher
  # What kind of ExecutionService to use
  executor = "fork-join-executor"
  # Configuration for the fork join pool
  fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 2
    # Parallelism (threads) ... ceil(available processors * factor)
    parallelism-factor = 2.0
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 20
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 1
}

health-monitor-dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    fixed-pool-size = 8
  }
  throughput = 1
}

gcs {
  defaultLocation = "us-central1"
  bucketLogsMaxAge = "180"
  google-api-uri = "https://www.googleapis.com"

  deploymentManager {
    #If you're working on DM and want to keep the deployments around for inspection after they're done, set this to false.
    #Don't set it to false in prod, ever, as there is a 1000 deployment limit and we'll hit it real quick without cleanup.
    cleanupDeploymentAfterCreating = true
  }
}

submissionmonitor {
  submissionPollInterval = 1m
  submissionPollExpiration = 30 days
  trackDetailedSubmissionMetrics = true
  attributeUpdatesPerWorkflow = 20000
  enableEmailNotifications = true
}

workspaceManagerResourceMonitor {
  defaultRetrySeconds = 4
  retryUncompletedJobsSeconds = 5
}

data-source {
  coordinated-access {
    # When true avoids certain database deadlock conditions by sending some data access transactions through an actor
    enabled = true
    # How long to wait for each transaction to start
    start-timeout = 1 minute
    # How long to wait for each transaction to run
    wait-timeout = 1 minute
    # How long to wait for each transaction to reply (incl. wait to start, and then run)
    ask-timeout = 3 minutes
  }
}

slick {
  # batchSize is used for writes, to group inserts/updates
  # this must be explicitly utilized via custom business logic
  batchSize = 2000
  # fetchSize is used during Slick streaming to set the size of pages
  # this must be explicitly set via withStatementParameters
  fetchSize = 5000
}

wdl-parsing {
  # number of parsed WDLs to cache
  cache-max-size = 7500
  # TTL for WDLs where the parser returned normally
  # 432000 seconds = 5 days
  cache-ttl-success-seconds = 432000
  # TTL for WDLs where the parser encountered a transient/retryable error, such as a timeout.
  # Set this to zero to not cache these failures. Set this to a low number to return the cached failure
  # to any queued threads waiting on WDL parsing, and thus allow the queue to drain quickly instead
  # of backing up on a slow error
  cache-ttl-failure-seconds = 2
  # timeout for WDL parsing
  parser-thread-pool-timeout-seconds = 50
  server = "fake/path"
  # Whether or not to cache validation responses from Cromwell
  useCache = true
}

entityUpsert {
  maxContentSizeBytes = 67108864
}

entityStatisticsCache {
  enabled = true
  timeoutPerWorkspace = 3 minutes
  standardPollInterval = 1 minute
  workspaceCooldown = 4 minutes
}

entities {
  pageSizeLimit = 300000
}

akka.http.host-connection-pool.max-open-requests = 16384
akka.http.host-connection-pool.max-connections = 20
akka.http.server.idle-timeout = 210 s
akka.http.server.request-timeout=180 s

dataRepoEntityProvider {
  # 10 expressions * 100000 base table rows
  maxInputsPerSubmission = 1000000
  maxBigQueryResponseSize = 500MB
  bigQueryMaximumBytesBilled = 100GB
}

leonardo {
    wdsType = "CROMWELL"
    server = "https://leonardo.dsde-dev.broadinstitute.org"
}

sam.timeout = 3 minutes

prometheus {
  endpointPort = 9098
}
